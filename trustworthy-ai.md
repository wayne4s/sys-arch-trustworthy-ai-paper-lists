# Paper Lists for Trustworthy AI

## Table of Contents
 - [Survey](#survey)
 - [AI Safety](#ai-safety)
    - [Alignment](#alignment)
    - [Hallucination](#hallucination)
    - [Fairness-Ethics](#fairness-ethics)
 - [AI Security](#ai-security)
    - [Jailbreak Attacks](#jailbreak-attacks)
    - [Misinformation Attacks](#misinformation-attacks)
    - [Prompt Attacks](#prompt-attacks)
    - [Privacy-preserving Computation](#privacy-preserving-computation)


## Survey

- [x] [2025 arXiv] **AI Safety vs. AI Security: Demystifying the Distinction and Boundaries.** [![](https://img.shields.io/badge/paper-7EA6E0)](https://arxiv.org/abs/2506.18932) [![](https://img.shields.io/badge/slides-E29135)](https://zhiqlin.github.io/file/talks/AI_Safety_Security_July_17_2025.pdf) [![](https://img.shields.io/badge/article-719AAC)](https://mp.weixin.qq.com/s/7k6RR4BMl7gcROFfzWhJjg)

- [x] [2025 arXiv] **A Comprehensive Survey in LLM(-Agent) Full Stack Safety: Data, Training and Deployment.** [![](https://img.shields.io/badge/paper-7EA6E0)](https://arxiv.org/abs/2504.15585) [![](https://img.shields.io/badge/article-719AAC)](https://mp.weixin.qq.com/s/ym-Bv1tPs57Y3zI-Pu6lzA)

- [x] **A Survey on Trustworthy LLM Agents: Threats and Countermeasures.** [![](https://img.shields.io/badge/paper-7EA6E0)](https://arxiv.org/abs/2503.09648)

- [x] **Large Model Based Agents: State-of-the-Art, Cooperation Paradigms, Security and Privacy, and Future Trends.**  [![](https://img.shields.io/badge/paper-7EA6E0)](https://arxiv.org/abs/2409.14457)

- [x] [2025 arXiv] **Safety at Scale: A Comprehensive Survey of Large Model and Agent Safety.** [![](https://img.shields.io/badge/paper-7EA6E0)](https://arxiv.org/abs/2502.05206) [![](https://img.shields.io/badge/article-719AAC)](https://mp.weixin.qq.com/s/CKVe-45__NFey16gex55zQ) [![](https://img.shields.io/badge/homepage-808080)](https://github.com/xingjunm/Awesome-Large-Model-Safety?tab=readme-ov-file) 

- [x] **Large model agents: State-of-theart, cooperation paradigms, security and privacy, and future trends.** [![](https://img.shields.io/badge/paper-7EA6E0)](https://arxiv.org/abs/2409.14457)


- [x] **Towards Efficient Privacy-Preserving Machine Learning: A Systematic Review from Protocol, Model, and System Perspectives.** (*PKU*) [![](https://img.shields.io/badge/paper-7EA6E0)](https://arxiv.org/pdf/2507.14519) [![](https://img.shields.io/badge/article-719AAC)](https://www.51cto.com/article/822336.html) [![](https://img.shields.io/badge/code-B5739D)](https://github.com/PKU-SEC-Lab/Awesome-PPML-Papers)

- [ ] **TextAttack: A Framework for Adversarial Attacks, Data Augmentation, and Adversarial Training in NLP.** [![](https://img.shields.io/badge/paper-7EA6E0)](https://arxiv.org/abs/2005.05909) [![](https://img.shields.io/badge/code-B5739D)](https://github.com/QData/TextAttack?tab=readme-ov-file)

## AI Safety
<!-- ### Alignment -->
***Alignment***

- [ ] [2025 arXiv] **STAIR: Improving Safety Alignment with Introspective Reasoning.** [![](https://img.shields.io/badge/paper-7EA6E0)](https://arxiv.org/abs/2502.02384) [![](https://img.shields.io/badge/article-719AAC)](https://mp.weixin.qq.com/s/1v4A6JBDSTrcw1nGnRR4ow) [![](https://img.shields.io/badge/code-B5739D)](https://github.com/thu-ml/STAIR)

- [ ] [2025 arXiv] **RealSafe-R1: Safety-Aligned DeepSeek-R1 without Compromising Reasoning Capability.** [![](https://img.shields.io/badge/paper-7EA6E0)](https://arxiv.org/abs/2504.10081) [![](https://img.shields.io/badge/article-719AAC)](https://mp.weixin.qq.com/s/1v4A6JBDSTrcw1nGnRR4ow) [![](https://img.shields.io/badge/code-B5739D)](https://huggingface.co/RealSafe)
- [ ] [2025 arXiv] **ERPO: Advancing Safety Alignment via Ex-Ante Reasoning Preference Optimization.** [![](https://img.shields.io/badge/paper-7EA6E0)](https://arxiv.org/abs/2504.02725)
- [ ] **Safety Alignment Should Be Made More Than Just a Few Tokens Deep.** [![](https://img.shields.io/badge/paper-7EA6E0)](https://arxiv.org/abs/2406.05946) [![](https://img.shields.io/badge/code-B5739D)](https://github.com/Unispac/shallow-vs-deep-alignment)

<!-- ### Hallucination -->
***Hallucination***

<!-- ### Fairness & Ethics -->
***Fairness & Ethics***


## AI Security


<!-- ### Jailbreak Attacks -->
***Jailbreak Attacks***

- [ ] [2024 NeurIPS] **Bag of Tricks: Benchmarking of Jailbreak Attacks on LLMs.** [![](https://img.shields.io/badge/paper-7EA6E0)](https://proceedings.neurips.cc/paper_files/paper/2024/file/38c1dfb4f7625907b15e9515365e7803-Paper-Datasets_and_Benchmarks_Track.pdf) [![](https://img.shields.io/badge/article-719AAC)](https://mp.weixin.qq.com/s/KulCxJm1wgz2fqorfuJ3Iw) [![](https://img.shields.io/badge/code-B5739D)](https://github.com/usail-hkust/JailTrickBench.git)


- [ ] [2025 ACL] **Jailbreak Large Vision-Language Models Through Multi-Modal Linkage.** [![](https://img.shields.io/badge/paper-7EA6E0)](https://aclanthology.org/2025.acl-long.74.pdf)

- [ ] [2025 ACL] **Shaping the Safety Boundaries: Understanding and Defending Against Jailbreaks in Large Language Models.** [![](https://img.shields.io/badge/paper-7EA6E0)](https://aclanthology.org/2025.acl-long.1233.pdf)

- [ ] [2024 arXiv] **Heuristic-Induced Multimodal Risk Distribution Jailbreak Attack for Multimodal Large Language Models.** [![](https://img.shields.io/badge/paper-7EA6E0)](https://arxiv.org/abs/2412.05934) [![](https://img.shields.io/badge/article-719AAC)](https://mp.weixin.qq.com/s/XqsQE_tdA4gmlsDjp4pHtA)

- [ ] [2025 arXiv] **Implicit Jailbreak Attacks via Cross-Modal Information Concealment on Vision-Language Models.** [![](https://img.shields.io/badge/paper-7EA6E0)](https://arxiv.org/abs/2505.16446v1)

- [ ] [2025 Security] **SelfDefend: LLMs Can Defend Themselves against Jailbreaking in a Practical Manner.** [![](https://img.shields.io/badge/paper-7EA6E0)](https://daoyuan14.github.io/papers/USENIX25_SelfDefend.pdf)

<!-- ### Misinformation Attacks -->
***Misinformation Attacks***

- [ ] [2024 NPJ Digital Medicine] **Medical large language models are susceptible to targeted misinformation attacks.** [![](https://img.shields.io/badge/paper-7EA6E0)](https://www.nature.com/articles/s41746-024-01282-7)



<!-- ### Prompt Attacks -->
***Prompt Injection***

- [x] **Not what you’ve signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection.** [![](https://img.shields.io/badge/paper-7EA6E0)](https://arxiv.org/abs/2302.12173) [![](https://img.shields.io/badge/code-B5739D)](https://github.com/greshake/llm-security)

- [ ] **More than you've asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models.** [![](https://img.shields.io/badge/paper-7EA6E0)](https://arxiv.org/abs/2302.12173) [![](https://img.shields.io/badge/code-B5739D)](https://github.com/greshake/llm-security/tree/main?tab=readme-ov-file)

- [ ] **StruQ: Defending Against Prompt Injection with Structured Queries.** [![](https://img.shields.io/badge/paper-7EA6E0)](https://www.usenix.org/conference/usenixsecurity25/presentation/chen-sizhe) [![](https://img.shields.io/badge/slides-E29135)](https://drive.google.com/file/d/1baUbgFMILhPWBeGrm67XXy_H-jO7raRa/view) [![](https://img.shields.io/badge/article-719AAC)](https://bair.berkeley.edu/blog/2025/04/11/prompt-injection-defense/) [![](https://img.shields.io/badge/code-B5739D)](https://github.com/Sizhe-Chen/StruQ)

- [ ] [2024 arXiv] **Prompt Injection attack against LLM-integrated Applications.** [![](https://img.shields.io/badge/paper-7EA6E0)](https://arxiv.org/abs/2306.05499) [![](https://img.shields.io/badge/code-B5739D)](https://github.com/LLMSecurity/HouYi)


- [ ] **Monitoring Reasoning Models for Misbehavior and the Risks of Promoting Obfuscation.** [![](https://img.shields.io/badge/paper-7EA6E0)](https://arxiv.org/abs/2503.11926) [![](https://img.shields.io/badge/article-719AAC)](https://lilianweng.github.io/posts/2025-05-01-thinking/)

- [ ] [2025 S&P] **Prompt Inversion Attack against Collaborative Inference of Large Language Models.** [![](https://img.shields.io/badge/code-B5739D)](https://github.com/ygZhou02/DEML)


<!-- ### Extraction Attacks -->
***Extraction***

- [ ] [2021 Security] **Extracting Training Data from Large Language Models.** [![](https://img.shields.io/badge/paper-7EA6E0)](https://www.usenix.org/system/files/sec21-carlini-extracting.pdf)

- [ ] [2025 ACL] **VLSBench: Unveiling Information Leakage in Multimodal Safety.** [![](https://img.shields.io/badge/paper-7EA6E0)](https://aclanthology.org/2025.acl-long.405.pdf) [![](https://img.shields.io/badge/code-B5739D)](https://github.com/AI45Lab/VLSBench)

<!-- ### Adversarial Attacks -->
***Adversarial***

- [ ] **Adversarial Attacks on LLMs.**  [![](https://img.shields.io/badge/article-719AAC)](https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/)


- [ ] **TextAttack: A Framework for Adversarial Attacks, Data Augmentation, and Adversarial Training in NLP.** [![](https://img.shields.io/badge/paper-7EA6E0)](https://arxiv.org/abs/2005.05909) [![](https://img.shields.io/badge/code-B5739D)](https://github.com/QData/TextAttack?tab=readme-ov-file)


<!-- ### Backdoor Attack and Defense -->
***Backdoor***

- [ ] **BackdoorBox: An Open-sourced Python Toolbox for Backdoor Attacks and Defenses.** [![](https://img.shields.io/badge/paper-7EA6E0)](https://arxiv.org/pdf/2302.01762) [![](https://img.shields.io/badge/code-B5739D)](https://github.com/THUYimingLi/BackdoorBox)

- [ ] **Backdoor Toolbox.** [![](https://img.shields.io/badge/code-B5739D)](https://github.com/vtu81/backdoor-toolbox)

---

### Privacy-preserving Computation
<!-- 真正实现“数据可用不可见”的安全愿景。-->
<!-- 在保证数据提供方不泄露原始数据的前提下，对数据进行分析计算的一系列信息技术，保障数据在流通与融合过程中的“可用不可见”。 -->
<!-- 将隐私计算相关技术概括为三个大类，分别为以安全多方计算为代表的密码学路径、以可信任执行环境为代表的硬件路径和以联邦学习为代表的人工智能路径 -->


<!-- #### Secure Multi-Party Computation (SMPC) -->
***Secure Multi-Party Computation (SMPC)***

<!-- 多方安全计算指参与者在不泄露各自隐私数据情况下，利用隐私数据参与保密计算，共同完成某项计算任务。 -->

- [ ] ![](https://img.shields.io/badge/READ-EA6B66) **MPCache: MPC-Friendly KV Cache Eviction for Efficient Private Large Language Model Inference.** [![](https://img.shields.io/badge/paper-7EA6E0)](https://arxiv.org/abs/2501.06807)
- [ ] **Secure Multiparty Generative AI.** [![](https://img.shields.io/badge/paper-7EA6E0)](https://arxiv.org/abs/2409.19120)
- [ ] **MPC-Minimized Secure LLM Inference.** [![](https://img.shields.io/badge/paper-7EA6E0)](https://arxiv.org/abs/2408.03561)
- [2024 ACL] **SecFormer: Fast and Accurate Privacy-Preserving Inference for Transformer Models via SMPC.** [![](https://img.shields.io/badge/paper-7EA6E0)](https://aclanthology.org/2024.findings-acl.790/)


<!-- #### Homomorphic Encryption -->
***Homomorphic Encryption***

- [ ] [2025 DAC] **ABC-FHE: A Resource-Efficient Accelerator Enabling Bootstrappable Parameters for Client-Side Fully Homomorphic Encryption.** [![](https://img.shields.io/badge/paper-7EA6E0)](https://arxiv.org/abs/2506.08461)
- [ ] **Encryption-Friendly LLM Architecture.** [![](https://img.shields.io/badge/paper-7EA6E0)](https://arxiv.org/abs/2410.02486)
- [ ] [2025 TACO] **Matrix: Multi-Cipher Structures Dataflow for Parallel and Pipelined TFHE Accelerator.** 

<!-- #### Differential Privacy (DP) -->
***Differential Privacy (DP)***
<!-- > Add calibrated noise to data or model outputs, preventing attackers from inferring whether specific individuals were included in the training dataset. -->
- [ ] **Differentially Private Steering for Large Language Model Alignment.** [![](https://img.shields.io/badge/paper-7EA6E0)](https://arxiv.org/abs/2501.18532)

<!-- #### Federated Learning (FL) -->
***Federated Learning (FL)***
<!-- > A distributed machine learning approach where raw data remains on local devices, and only model updates are aggregated centrally, preserving data privacy without compromising model performance. -->
<!-- 联邦学习是一种分布式机器学习技术，通过在多个拥有本地数据的数据源之间进行分布式模型训练；在不需要交换本地个体或样本数据的前提下，仅通过交换模型参数或中间结果的方式构建基于虚拟融合数据下的全局模型，从而实现数据隐私保护和数据共享计算的平衡，即“数据可用不可见”、“数据不动模型动”的应用新范式。 -->
- [ ] **Federated In-Context LLM Agent Learning.** [![](https://img.shields.io/badge/paper-7EA6E0)](https://arxiv.org/abs/2412.08054)


<!-- #### Trusted Execution Environments (TEE)  -->
***Trusted Execution Environments (TEE)***
<!-- 可信执行环境是一种能确保代码和数据在执行过程中安全、完整且不被篡改的环境。 -->

<!-- #### Zero-Knowledge Proofs (ZKP) -->
***Zero-Knowledge Proofs (ZKP)***
<!-- > Cryptographic protocols that allow one party to prove knowledge of a secret without revealing the secret itself, enabling privacy-preserving verification and authentication. -->

- [ ] [2025 DAC] **zkVC: Fast Zero-Knowledge Proof for Private and Verifiable Computing.** [![](https://img.shields.io/badge/paper-7EA6E0)](https://arxiv.org/abs/2504.12217) [![](https://img.shields.io/badge/code-B5739D)](https://github.com/UCF-Lou-Lab-PET/zkformer)

<!--
- [ ] [xxx] **xxx.** 
[![](https://img.shields.io/badge/paper-7EA6E0)]
[![](https://img.shields.io/badge/article-719AAC)]()
[![](https://img.shields.io/badge/slides-E29135)]()
[![](https://img.shields.io/badge/code-B5739D)]()
![](https://img.shields.io/badge/READ-EA6B66)
-->
